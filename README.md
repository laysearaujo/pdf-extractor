# üöÄ Smart PDF Extractor

Esta √© uma solu√ß√£o de extra√ß√£o de dados de PDFs que implementa uma arquitetura de auto-aprendizagem para otimizar o equil√≠brio entre custo, velocidade e precis√£o.

O sistema aprende com a primeira extra√ß√£o de um novo tipo de documento (definido por um label) e cria heur√≠sticas (√Çncoras, Zonas e Regex) para tornar todas as extra√ß√µes futuras desse mesmo `label` instant√¢neas e **sem custo de LLM**.

√â importante notar que a primeira requisi√ß√£o de um novo `label` (N√≠vel 1) sempre ser√° mais lenta, pois exige uma chamada √† API do LLM para o aprendizado. No entanto, as requisi√ß√µes subsequentes para esse label tendem a ser processadas localmente (N√≠vel 2), tornando-as quase instant√¢neas ou com menos informa√ß√µes a serem processadas no llm (maior rapidez), o que permite que a m√©dia de processamento em lote atenda ao objetivo de <10s por documento.

## üß† Arquitetura de Auto-Aprendizagem

O desafio principal n√£o √© apenas extrair dados, mas faz√™-lo de forma eficiente, lidando com layouts fixos e vari√°veis, aprendendo com o tempo e otimizando o custo de m√∫ltiplas chamadas de API. A arquitetura aprende com o LLM e se autocorrige, operando em tr√™s n√≠veis:

### N√≠vel 1: Bootstrap (O Dilema do LLM)

* **Desafio**: Chamar um LLM (como o `gpt-5-mini`) para cada PDF √© caro, lento e um desperd√≠cio, especialmente para documentos com layouts fixos.

* **Solu√ß√£o Proposta**: Uma arquitetura de auto-aprendizagem baseada em label.

   * O LLM (o "c√©rebro" caro) √© usado **apenas uma vez** por label, na fun√ß√£o `_bootstrap_new_label_with_llm`.

   * Esta fun√ß√£o usa o LLM para extrair os dados e tamb√©m para classificar o template como `"template_fixo": true` ou `false`.

      Se for fixo, o sistema aprende e salva heur√≠sticas (√Çncora, Zona ou Regex) num ficheiro `knowledge_base.json`.

   * Todas as extra√ß√µes futuras com o mesmo label **usam estas heur√≠sticas de custo zero, tornando-as quase instant√¢neas**.

### N√≠vel 2: Extra√ß√£o por Heur√≠stica (Custo Zero)

* **Desafio**: Heur√≠sticas s√£o fr√°geis. Uma √¢ncora pode falhar por causa de um acento (`Inscri√ß√£o` vs `Inscricao`), e uma zona pode cortar palavras (`LUIS FILIPE A` em vez de `LUIS FILIPE ARAUJO AMARAL`).

* **Solu√ß√£o Proposta**: O sistema usa um conjunto de heur√≠sticas inteligentes e robustas:

   1. **Busca Normalizada R√°pida**: Foi criada uma fun√ß√£o `_search_for_normalized` que pr√©-processa todas as palavras do PDF uma vez (`parsed_pdf_cache`). Esta busca ignora acentos e capitaliza√ß√£o (ex: '√ß' == 'C'), tornando as √¢ncoras 99% mais robustas.

   2. **Aprendizado de √Çncora (4 Dire√ß√µes)**: A fun√ß√£o `_derive_heuristic_for_value` n√£o procura √¢ncoras s√≥ "acima" ou "√† esquerda", mas em todas as 4 dire√ß√µes (acima, abaixo, esquerda, direita), tornando a heur√≠stica de `ANCHOR` muito mais prov√°vel de ser encontrada.

   3. **Aprendizado de Regex**: Ao aprender, `_guess_regex_for_value` tenta adivinhar um padr√£o para o valor (ex: `\d{2}/\d{2}/\d{4}` para datas, `\d{3}\.\d{3}\.\d{3}-\d{2}` para CPFs).

   4. **Extra√ß√£o Precisa (com Regex)**: Se um Regex foi aprendido, as fun√ß√µes `_apply_anchor_heuristic` e `_apply_zone_heuristic` o utilizam para filtrar o texto extra√≠do, garantindo que apenas o valor no formato correto seja retornado.

   5. **Gest√£o de Nulos**: O sistema aprende a regra `ANCHOR_EMPTY` para campos que existem mas est√£o vazios (ex: telefone_profissional), evitando chamar o LLM desnecessariamente.

### N√≠vel 3: Fallback e Autocorre√ß√£o (Processamento Eficiente)

Este n√≠vel √© acionado quando uma heur√≠stica conhecida (N√≠vel 2) falha.

1. O sistema itera por cada PDF individualmente. Para cada um, ele primeiro tenta extrair todos os campos usando as heur√≠sticas de Custo Zero.

2. Todos os campos que falham (em um √∫nico PDF) s√£o adicionados a uma lista de falhas tempor√°ria (`_failed_fields`) para aquele documento.

3. Se houver qualquer falha, a fun√ß√£o `_single_doc_llm_fallback` √© chamada uma √∫nica vez para aquele PDF.

4. Esta fun√ß√£o envia todas as falhas daquele documento (ex: "nome", "inscricao") num "prompt massivo" √∫nico para o LLM.

5. Os resultados retornados pelo LLM s√£o usados para a autocorre√ß√£o, chamando `_derive_heuristic_for_value` para aprender uma nova heur√≠stica (agora mais inteligente) e substituindo a antiga no KB.

## üöÄ Como Utilizar

A solu√ß√£o √© entregue como uma aplica√ß√£o web Flask (`app.py`) que serve uma UI simples (index.html) e exp√µe endpoints de API. O processamento em lote √© feito atrav√©s de um script de cliente (`batch_extract.py`).

1. **Pr√©-requisitos**
   * Python 3.12+
   * Chave da API da OpenAI

2. **Instala√ß√£o**

   * Clone o reposit√≥rio.

   * Crie um ambiente virtual: `python -m venv venv` e `source venv/bin/activate`

   * Instale as depend√™ncias: `pip install -r requirements.txt`

   * Defina sua chave de API (escolha uma):

      * M√©todo A (Bash): `export OPENAI_API_KEY='sk-...'`

      * M√©todo B (.env): Crie um arquivo `.env` no diret√≥rio e adicione `OPENAI_API_KEY='sk-...'`.

3. **Executando a Aplica√ß√£o (UI + API)**

   ```Bash
   python app.py
   ```
   A aplica√ß√£o estar√° dispon√≠vel em http://127.0.0.1:8000.

4. **Usando a Interface Web (UI)**

Acesse `http://127.0.0.1:8000` no seu navegador. A UI tem tr√™s abas:

   * **Extra√ß√£o √önica**: Permite enviar um √∫nico PDF com um `label` e `schema` para teste r√°pido.

   * **Extra√ß√£o em Lote**: Permite fazer o upload de um ficheiro JSON de pedidos e dos m√∫ltiplos PDFs correspondentes, e depois baixar os resultados.

   * **Instru√ß√µes da API**: Mostra exemplos `curl` para usar a API diretamente.

5. **Usando o Script de Lote (Recomendado)**

Esta √© a forma mais poderosa de usar a solu√ß√£o.

1. **Crie o seu ficheiro de pedidos** (ex: `example_requests.json`): O `pdf_path` √© o **ID** (nome do ficheiro) que o script ir√° procurar na sua pasta de PDFs.

```JSON

[
  {
    "label": "carteira_oab",
    "extraction_schema": { "nome": "Nome do profissional", "inscricao": "N√∫mero" },
    "pdf_path": "oab_1.pdf"
  },
  {
    "label": "tela_sistema",
    "extraction_schema": { "produto": "Produto da opera√ß√£o" },
    "pdf_path": "tela_1.pdf"
  },
  {
    "label": "carteira_oab",
    "extraction_schema": { "nome": "Nome", "seccional": "Seccional" },
    "pdf_path": "oab_2.pdf"
  }
]
```

2. **Coloque os seus PDFs numa pasta**:

```bash
/meus_pdfs/
‚îú‚îÄ‚îÄ oab_1.pdf
‚îú‚îÄ‚îÄ oab_2.pdf
‚îî‚îÄ‚îÄ tela_1.pdf
```

3. **Execute o script**: O script `batch_extract.py` envia o JSON e todos os PDFs encontrados na pasta para a API `/api/batch_upload`.

```Bash
# python batch_extract.py [CAMINHO_JSON] [CAMINHO_PASTA_PDFS]
python batch_extract.py ./example_requests.json ./meus_pdfs/
```

**4. Receba os Resultados**:

O script ir√° imprimir os resultados no terminal e tamb√©m salvar√° automaticamente um ficheiro `extraction_results.json` no seu diret√≥rio.

#### ‚ö†Ô∏è Limita√ß√µes Conhecidas
   Velocidade do LLM: O desempenho do N√≠vel 1 (Bootstrap) e N√≠vel 3 (Fallback) est√° diretamente atrelado √† velocidade de resposta do gpt-5-mini. O modelo √© configurado com temperature: 1.0, pois o gpt-5-mini pode n√£o suportar temperature: 0 (o que seria ideal para extra√ß√£o determin√≠stica e mais r√°pida). Isso adiciona lat√™ncia e variabilidade √†s opera√ß√µes de aprendizado e autocorre√ß√£o.

## üèõÔ∏è Arquitetura Alternativa (Com Async e WebSockets)

A arquitetura atual usa `threading` no `app.py` para criar um job em segundo plano e um sistema de polling (consultas repetidas) no endpoint `/api/batch_status/<job_id>` para verificar o progresso. Esta √© uma solu√ß√£o robusta e cl√°ssica para o Flask.

Se a stack tecnol√≥gica permitisse `async` nativo (usando frameworks como FastAPI ou Quart), uma arquitetura ainda mais perform√°tica seria poss√≠vel:

* **API N√£o-Bloqueante**: O endpoint `/api/batch_upload` seria async e, em vez de threading, usaria BackgroundTasks (FastAPI) ou um sistema de fila dedicado para iniciar o processamento sem bloquear o servidor.

* **Progresso em Tempo Real com WebSockets**: Em vez de o cliente perguntar ao servidor "j√° terminou?" a cada 2 segundos (polling), o cliente abriria uma conex√£o WebSocket. O servidor, ent√£o, empurraria atualiza√ß√µes de status para o cliente em tempo real (ex: "processado: 5/100", "processado: 6/100"), eliminando a necessidade de polling.

* **Extrator Concorrente**: O SmartExtractor poderia usar um cliente `AsyncOpenAI`. A maior vantagem estaria no `_single_doc_llm_fallback`: se 10 PDFs em um lote precisarem de fallback, em vez de process√°-los sequencialmente (esperando 5-10s por cada um), um extrator async poderia executar todas as 10 chamadas de LLM concorrentemente com `asyncio.gather()`, reduzindo o tempo de espera de 100 segundos para ~10 segundos.
